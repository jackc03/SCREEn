---------------------------------------
Begin Slurm Prolog: Apr-28-2025 18:10:35
Job ID:    2543750
User ID:   jcochran66
Account:   ece
Job name:  TrainingFACE
Partition: coe-gpu
QOS:       coe-ice
---------------------------------------
Mon Apr 28 18:10:39 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 555.42.02              Driver Version: 555.42.02      CUDA Version: 12.5     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100 80GB HBM3          On  |   00000000:40:00.0 Off |                    0 |
| N/A   32C    P0            102W /  700W |       1MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  |   00000000:49:00.0 Off |                    0 |
| N/A   34C    P0             99W /  700W |       1MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|  No running processes found                                                             |
+-----------------------------------------------------------------------------------------+
W0428 18:10:41.440000 622863 /storage/ice1/4/8/jcochran66/.conda/envs/sr_design/lib/python3.12/site-packages/torch/distributed/run.py:792] 
W0428 18:10:41.440000 622863 /storage/ice1/4/8/jcochran66/.conda/envs/sr_design/lib/python3.12/site-packages/torch/distributed/run.py:792] *****************************************
W0428 18:10:41.440000 622863 /storage/ice1/4/8/jcochran66/.conda/envs/sr_design/lib/python3.12/site-packages/torch/distributed/run.py:792] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0428 18:10:41.440000 622863 /storage/ice1/4/8/jcochran66/.conda/envs/sr_design/lib/python3.12/site-packages/torch/distributed/run.py:792] *****************************************
[rank1]: Traceback (most recent call last):
[rank1]:   File "/home/hice1/jcochran66/code/SCREEn/run_screen.py", line 239, in <module>
[rank1]:     main()
[rank1]:   File "/home/hice1/jcochran66/code/SCREEn/run_screen.py", line 209, in main
[rank1]:     sr   = model(prev, cur, nxt)
[rank1]:            ^^^^^^^^^^^^^^^^^^^^^
[rank1]:   File "/home/hice1/jcochran66/.conda/envs/sr_design/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
[rank1]:     return self._call_impl(*args, **kwargs)
[rank1]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank1]:   File "/home/hice1/jcochran66/.conda/envs/sr_design/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
[rank1]:     return forward_call(*args, **kwargs)
[rank1]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank1]:   File "/home/hice1/jcochran66/.conda/envs/sr_design/lib/python3.12/site-packages/torch/nn/parallel/distributed.py", line 1643, in forward
[rank1]:     else self._run_ddp_forward(*inputs, **kwargs)
[rank1]:          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank1]:   File "/home/hice1/jcochran66/.conda/envs/sr_design/lib/python3.12/site-packages/torch/nn/parallel/distributed.py", line 1459, in _run_ddp_forward
[rank1]:     return self.module(*inputs, **kwargs)  # type: ignore[index]
[rank1]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank1]:   File "/home/hice1/jcochran66/.conda/envs/sr_design/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
[rank1]:     return self._call_impl(*args, **kwargs)
[rank1]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank1]:   File "/home/hice1/jcochran66/.conda/envs/sr_design/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
[rank1]:     return forward_call(*args, **kwargs)
[rank1]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank1]:   File "/home/hice1/jcochran66/code/SCREEn/screen.py", line 73, in forward
[rank1]:     feat1080 = self.rec1080(feat1080)                   # (B,16,1080,1920)
[rank1]:                ^^^^^^^^^^^^^^^^^^^^^^
[rank1]:   File "/home/hice1/jcochran66/.conda/envs/sr_design/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
[rank1]:     return self._call_impl(*args, **kwargs)
[rank1]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank1]:   File "/home/hice1/jcochran66/.conda/envs/sr_design/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
[rank1]:     return forward_call(*args, **kwargs)
[rank1]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank1]:   File "/home/hice1/jcochran66/code/SCREEn/screen.py", line 38, in forward
[rank1]:     x = cell(x, None)
[rank1]:         ^^^^^^^^^^^^^
[rank1]:   File "/home/hice1/jcochran66/.conda/envs/sr_design/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
[rank1]:     return self._call_impl(*args, **kwargs)
[rank1]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank1]:   File "/home/hice1/jcochran66/.conda/envs/sr_design/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
[rank1]:     return forward_call(*args, **kwargs)
[rank1]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank1]:   File "/home/hice1/jcochran66/code/SCREEn/screen.py", line 22, in forward
[rank1]:     n = torch.tanh(self.out(torch.cat([x, r * h], 1)))
[rank1]:                             ^^^^^^^^^^^^^^^^^^^^^^^^
[rank1]: torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 7.91 GiB. GPU 1 has a total capacity of 79.22 GiB of which 3.83 GiB is free. Including non-PyTorch memory, this process has 75.38 GiB memory in use. Of the allocated memory 63.73 GiB is allocated by PyTorch, and 10.10 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[rank0]: Traceback (most recent call last):
[rank0]:   File "/home/hice1/jcochran66/code/SCREEn/run_screen.py", line 239, in <module>
[rank0]:     main()
[rank0]:   File "/home/hice1/jcochran66/code/SCREEn/run_screen.py", line 209, in main
[rank0]:     sr   = model(prev, cur, nxt)
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/home/hice1/jcochran66/.conda/envs/sr_design/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
[rank0]:     return self._call_impl(*args, **kwargs)
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/home/hice1/jcochran66/.conda/envs/sr_design/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
[rank0]:     return forward_call(*args, **kwargs)
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/home/hice1/jcochran66/.conda/envs/sr_design/lib/python3.12/site-packages/torch/nn/parallel/distributed.py", line 1643, in forward
[rank0]:     else self._run_ddp_forward(*inputs, **kwargs)
[rank0]:          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/home/hice1/jcochran66/.conda/envs/sr_design/lib/python3.12/site-packages/torch/nn/parallel/distributed.py", line 1459, in _run_ddp_forward
[rank0]:     return self.module(*inputs, **kwargs)  # type: ignore[index]
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/home/hice1/jcochran66/.conda/envs/sr_design/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
[rank0]:     return self._call_impl(*args, **kwargs)
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/home/hice1/jcochran66/.conda/envs/sr_design/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
[rank0]:     return forward_call(*args, **kwargs)
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/home/hice1/jcochran66/code/SCREEn/screen.py", line 73, in forward
[rank0]:     feat1080 = self.rec1080(feat1080)                   # (B,16,1080,1920)
[rank0]:                ^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/home/hice1/jcochran66/.conda/envs/sr_design/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
[rank0]:     return self._call_impl(*args, **kwargs)
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/home/hice1/jcochran66/.conda/envs/sr_design/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
[rank0]:     return forward_call(*args, **kwargs)
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/home/hice1/jcochran66/code/SCREEn/screen.py", line 38, in forward
[rank0]:     x = cell(x, None)
[rank0]:         ^^^^^^^^^^^^^
[rank0]:   File "/home/hice1/jcochran66/.conda/envs/sr_design/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
[rank0]:     return self._call_impl(*args, **kwargs)
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/home/hice1/jcochran66/.conda/envs/sr_design/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
[rank0]:     return forward_call(*args, **kwargs)
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/home/hice1/jcochran66/code/SCREEn/screen.py", line 22, in forward
[rank0]:     n = torch.tanh(self.out(torch.cat([x, r * h], 1)))
[rank0]:                             ^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]: torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 7.91 GiB. GPU 0 has a total capacity of 79.22 GiB of which 3.83 GiB is free. Including non-PyTorch memory, this process has 75.38 GiB memory in use. Of the allocated memory 63.73 GiB is allocated by PyTorch, and 10.10 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[rank0]:[W428 18:10:59.041067123 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
W0428 18:11:00.861000 622863 /storage/ice1/4/8/jcochran66/.conda/envs/sr_design/lib/python3.12/site-packages/torch/distributed/elastic/multiprocessing/api.py:897] Sending process 622887 closing signal SIGTERM
E0428 18:11:01.176000 622863 /storage/ice1/4/8/jcochran66/.conda/envs/sr_design/lib/python3.12/site-packages/torch/distributed/elastic/multiprocessing/api.py:869] failed (exitcode: 1) local_rank: 0 (pid: 622886) of binary: /home/hice1/jcochran66/.conda/envs/sr_design/bin/python
Traceback (most recent call last):
  File "/home/hice1/jcochran66/.conda/envs/sr_design/bin/torchrun", line 8, in <module>
    sys.exit(main())
             ^^^^^^
  File "/home/hice1/jcochran66/.conda/envs/sr_design/lib/python3.12/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 355, in wrapper
    return f(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^
  File "/home/hice1/jcochran66/.conda/envs/sr_design/lib/python3.12/site-packages/torch/distributed/run.py", line 918, in main
    run(args)
  File "/home/hice1/jcochran66/.conda/envs/sr_design/lib/python3.12/site-packages/torch/distributed/run.py", line 909, in run
    elastic_launch(
  File "/home/hice1/jcochran66/.conda/envs/sr_design/lib/python3.12/site-packages/torch/distributed/launcher/api.py", line 138, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hice1/jcochran66/.conda/envs/sr_design/lib/python3.12/site-packages/torch/distributed/launcher/api.py", line 269, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
run_screen.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2025-04-28_18:11:00
  host      : atl1-1-03-011-18-0.pace.gatech.edu
  rank      : 0 (local_rank: 0)
  exitcode  : 1 (pid: 622886)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
srun: error: atl1-1-03-011-18-0: task 0: Exited with exit code 1
---------------------------------------
Begin Slurm Epilog: Apr-28-2025 18:11:01
Job ID:        2543750
User ID:       jcochran66
Account:       ece
Job name:      TrainingFACE
Resources:     cpu=12,gres/gpu:h100=2,mem=264G,node=1
Rsrc Used:     cput=00:05:12,vmem=0,walltime=00:00:26,mem=37608K,energy_used=0
Partition:     coe-gpu
QOS:           coe-ice
Nodes:         atl1-1-03-011-18-0
---------------------------------------
