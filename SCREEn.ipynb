{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyPj/u4BGRjB0y7qFRFXSfkp",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jackc03/SCREEn/blob/colab_notebook/SCREEn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CRNN-Assisted Video Upscaling ASIC  \n",
        "### A Hardware/Software Codesign Walk-Through\n",
        "\n",
        "Welcome! This notebook is the companion journal for my **hardware/software co-design project**: an **ASIC accelerator that upgrades 720 p video streams to 1080 p in real time** using a **Convolutional Recurrent Neural Network (CRNN)**.  \n",
        "The goal is to show—step by step—how machine-learning research, algorithm engineering, RTL design, and physical-design constraints converge into a single silicon-ready pipeline.\n",
        "\n",
        "---\n",
        "\n",
        "## Motivation & Problem Statement\n",
        "- **Bandwidth bottleneck:** Mobile and embedded devices often downlink only 720 p to save bandwidth or storage.  \n",
        "- **Quality gap:** Naïve spatial upscalers (bilinear/nearest) yield soft edges and ringing artifacts.  \n",
        "- **Opportunity:** A compact CRNN can *learn* spatio-temporal correlations to hallucinate sharper textures, delivering near-native 1080 p quality at a fraction of the bitrate.  \n",
        "- **Challenge:** Deep models are compute-hungry. Achieving **⩾ 30 fps at 1080 p** within a **< 2 W power envelope** and **2 mm² core area** (SKY130 180 MHz budget) demands *co-optimized* hardware and software.\n",
        "\n",
        "---\n",
        "\n",
        "## High-Level Architecture\n",
        "| Stage | Function | Runs on |\n",
        "|-------|----------|---------|\n",
        "| **Pre-Upscale** | Bilinear 720 p → 1080 p (seed image) | On-chip DMA + line buffer |\n",
        "| **CRNN Core** | 5-layer Conv + gated recurrent loops | **Custom ASIC macro** |\n",
        "| **Post-Process** | Skip connection + tone mapping | **ASIC** |\n",
        "| **Runtime Driver** | Frame-DMA orchestration, quantized inference kernel, metrics | **RISC-V firmware** |\n",
        "\n",
        "A full RTL block diagram appears later in the notebook.\n",
        "\n",
        "---\n",
        "\n",
        "## Dataset & Training Recipe\n",
        "- **Dataset:** [DAVIS-2017 Unsupervised, Train/Val, Full-Resolution]—only raw RGB frames.  \n",
        "  *LR frames* are generated on the fly via bicubic ↓ in the `Dataset` class.  \n",
        "- **Loss mix:** Charbonnier (pixel) + temporal warping + GAN adversarial (`PatchGAN`, spectral-norm D).  \n",
        "- **Compression:** 4-bit weight quantization (PACT) + 8-bit activations, validated with < 0.2 dB PSNR drop.\n",
        "\n",
        "A reusable PyTorch pipeline is provided to replicate every experiment.\n",
        "\n",
        "---\n",
        "\n",
        "## Notebook Roadmap\n",
        "1. **Introduction** → this section!\n",
        "2. **Dataset setup** → download & prepare DAVIS-2017 for training\n",
        "3. **Model definition** → CRNN layers, quantization stubs  \n",
        "4. **Training loop** → adversarial curriculum, PSNR logger  \n",
        "5. **Hardware profiling** → MAC counts, SRAM fits, throughput model  \n",
        "6. **RTL generation** → Verilog modules, clock gating, synthesis (OpenROAD-Sky130)  \n",
        "7. **HW/SW integration** → RISC-V firmware, AXI-4 stream driver  \n",
        "8. **Results & discussion** → quality metrics, power/timing closure, future work\n",
        "\n",
        "---\n",
        "\n",
        "<!-- ## 5. How to Run\n",
        "```bash\n",
        "git clone <this-repo>\n",
        "cd notebook/\n",
        "pip install -r requirements.txt -->\n"
      ],
      "metadata": {
        "id": "DZ_zTXvvPyKw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2 Dataset Generation & Processing\n",
        "\n",
        "This section sets up everything we need to feed the **CRNN upscaler** with clean, memory-friendly training data.\n",
        "\n",
        "### 2.1 Source Material – DAVIS-2017 (Unsupervised, Full-Resolution)\n",
        "* • **60 train** + **30 val** video sequences, delivered as raw RGB frames  \n",
        "* • Stored under `datasets/DAVIS_4K/⟨seq⟩/*.jpg`.\n",
        "\n",
        "### 2.2 On-the-Fly LR/HR Pair Creation\n",
        "1. **Bicubic downscale** to (480 p, 1440 p) to create the LR, HR counterparts.  \n",
        "2. Assemble a **(prev, curr, next, hr) tuple** for temporal context.\n",
        "\n",
        "> *Why dynamic downscaling instead of stored LR copies?*  \n",
        "> Saves ~4 GB of disk, lets us experiment with different scale factors, and guarantees perfect alignment.\n",
        "\n",
        "### 2.3 DataLoader Blueprint\n",
        "| Split | # Sequences | # Triplets* | Purpose |\n",
        "|-------|-------------|------------:|---------|\n",
        "| **Train** | 60 | ≈ 25 k | Back-prop & augmentation |\n",
        "| **Val**   | 30 | ≈ 12 k | PSNR / SSIM checkpoints |\n",
        "| *(Test set loaded later for final metrics.)* |\n",
        "\n",
        "\\* Triplet count ≈ frames × (1 – 2/N) after dropping first & last frame per sequence.\n",
        "\n",
        "### 2.4 Sanity Checks\n",
        "* **Shape assert:** `(B, 3, H, W)` for each LR frame, `(B, 3, 2H, 2W)` for HR.  \n",
        "* **Quick PSNR** between bicubic LR↑ and HR to catch corrupted images.  \n",
        "* Visual spot-checks (overlay montage) stored in `/logs/sanity/`.\n",
        "\n",
        "---\n",
        "\n",
        "Run the next code cell to build the `VideoTripletDataset`, instantiate **train/val DataLoaders**, and print a mini-batch summary.\n"
      ],
      "metadata": {
        "id": "mMzW3b9LQjFt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "eiC68zjkVKXT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c0GBiKUYKtXq",
        "outputId": "b046097d-81d5-475a-c462-f888725d65a2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "/content/screen\n",
            "--2025-05-03 20:36:50--  https://data.vision.ee.ethz.ch/csergi/share/davis/DAVIS-2017-Unsupervised-trainval-Full-Resolution.zip\n",
            "Resolving data.vision.ee.ethz.ch (data.vision.ee.ethz.ch)... 129.132.52.178, 2001:67c:10ec:36c2::178\n",
            "Connecting to data.vision.ee.ethz.ch (data.vision.ee.ethz.ch)|129.132.52.178|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2957815900 (2.8G) [application/zip]\n",
            "Saving to: ‘datasets/DAVIS2017_Unsupervised_TrainVal_FR.zip’\n",
            "\n",
            "zip                  63%[===========>        ]   1.74G  16.3MB/s    eta 51s    "
          ]
        }
      ],
      "source": [
        "%cd /content/\n",
        "!rm -rf screen/\n",
        "# Create working directory\n",
        "!mkdir -p screen\n",
        "%cd screen\n",
        "\n",
        "\n",
        "# ─── DAVIS-2017 UNSUPERVISED Train+Val (Full-Res) ─────────────────────────\n",
        "!mkdir -p datasets\n",
        "FILE_URL=\"https://data.vision.ee.ethz.ch/csergi/share/davis/DAVIS-2017-Unsupervised-trainval-Full-Resolution.zip\"\n",
        "!wget -O datasets/DAVIS2017_Unsupervised_TrainVal_FR.zip \"$FILE_URL\"\n",
        "!unzip -q datasets/DAVIS2017_Unsupervised_TrainVal_FR.zip -d datasets/\n",
        "!rm datasets/DAVIS2017_Unsupervised_TrainVal_FR.zip\n",
        "!rm -rf datasets/DAVIS/Annotations_unsupervised/\n",
        "!rm -rf datasets/DAVIS/ImageSets/\n",
        "!rm -rf datasets/DAVIS/README.md\n",
        "!rm -rf datasets/DAVIS/SOURCES.md\n",
        "!mv datasets/DAVIS/JPEGImages/Full-Resolution/* datasets/DAVIS\n",
        "!rm -rf datasets/DAVIS/JPEGImages/\n",
        "!mv datasets/DAVIS datasets/DAVIS_4K"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python3\n",
        "\"\"\"\n",
        "prep_davis.py — create 480-p and 1440-p versions of every DAVIS-4K frame\n",
        "so that   (H480, W480) × 3  ==  (H1440, W1440).\n",
        "\n",
        "Layout produced:\n",
        "    datasets/\n",
        "        ├─ DAVIS_480/  <seq>/*.jpg\n",
        "        └─ DAVIS_1440/ <seq>/*.jpg\n",
        "\"\"\"\n",
        "import cv2, os, sys, math\n",
        "from pathlib import Path\n",
        "from tqdm import tqdm\n",
        "\n",
        "SRC_ROOT   = Path(\"datasets/DAVIS_4K\")\n",
        "DST_480    = Path(\"datasets/DAVIS_480\")\n",
        "DST_1440   = Path(\"datasets/DAVIS_1440\")\n",
        "\n",
        "# --------------------------------------------------------------------- #\n",
        "#  ↓ optional: pre-blur radius for very aggressive downscales (>×2)      #\n",
        "# --------------------------------------------------------------------- #\n",
        "GAUSS_PREBLUR = True        # set False to disable\n",
        "BLUR_SIGMA    = 0.7         # σ ≈ 0.7 gives good anti-alias\n",
        "\n",
        "def ensure_dir(p: Path):\n",
        "    if not p.exists():\n",
        "        p.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "def make_even(x: int) -> int:\n",
        "    return x + (x & 1)\n",
        "\n",
        "def dims_pair(h0: int, w0: int):\n",
        "    \"\"\"\n",
        "    Return (h480, w480, h1440, w1440) that keep aspect ratio and\n",
        "    satisfy (h1440, w1440) == 3 × (h480, w480).\n",
        "    \"\"\"\n",
        "    if h0 < w0:                                 # landscape\n",
        "        h480 = 480\n",
        "        w480 = int(round(w0 * h480 / h0))\n",
        "    else:                                       # portrait / square\n",
        "        w480 = 480\n",
        "        h480 = int(round(h0 * w480 / w0))\n",
        "\n",
        "    # enforce even sizes\n",
        "    h480 = make_even(h480)\n",
        "    w480 = make_even(w480)\n",
        "\n",
        "    h1440 = h480 * 3\n",
        "    w1440 = w480 * 3\n",
        "    return h480, w480, h1440, w1440\n",
        "\n",
        "def resize(img, new_size):\n",
        "    \"\"\"High-quality resize with optional pre-blur for strong downscales.\"\"\"\n",
        "    h_old, w_old = img.shape[:2]\n",
        "    w_new, h_new = new_size\n",
        "    if GAUSS_PREBLUR and (w_new < w_old or h_new < h_old):\n",
        "        # σ = k · sqrt( (scale^-2 − 1) ),  k ≈ 0.8  — loosely based on\n",
        "        # Mitchell–Netravali pre-filter heuristic\n",
        "        scale = min(w_new / w_old, h_new / h_old)\n",
        "        sigma = BLUR_SIGMA * math.sqrt(max(1/scale**2 - 1, 0))\n",
        "        if sigma > 0.1:\n",
        "            ksize = max(3, int(round(sigma * 3)) * 2 + 1)  # odd\n",
        "            img = cv2.GaussianBlur(img, (ksize, ksize), sigma)\n",
        "    interp = cv2.INTER_AREA if (w_new < w_old or h_new < h_old) else cv2.INTER_CUBIC\n",
        "    return cv2.resize(img, (w_new, h_new), interpolation=interp)\n",
        "\n",
        "# ------------------------------------------------------------------ #\n",
        "#  Main loop                                                         #\n",
        "# ------------------------------------------------------------------ #\n",
        "seq_dirs = [p for p in SRC_ROOT.iterdir() if p.is_dir()]\n",
        "print(f\"Found {len(seq_dirs)} sequences in {SRC_ROOT}\")\n",
        "\n",
        "for dst in (DST_480, DST_1440):\n",
        "    ensure_dir(dst)\n",
        "\n",
        "for seq in tqdm(seq_dirs, desc=\"Sequences\"):\n",
        "    ensure_dir(DST_480  / seq.name)\n",
        "    ensure_dir(DST_1440 / seq.name)\n",
        "\n",
        "    for img_path in seq.glob(\"*.jpg\"):\n",
        "        img = cv2.imread(str(img_path), cv2.IMREAD_COLOR)\n",
        "        if img is None:\n",
        "            print(f\"⚠️  Could not read {img_path}\", file=sys.stderr)\n",
        "            continue\n",
        "\n",
        "        h480, w480, h1440, w1440 = dims_pair(*img.shape[:2])\n",
        "\n",
        "        # ------------- 480-p -------------- #\n",
        "        out_480 = DST_480 / seq.name / img_path.name\n",
        "        if not out_480.exists() or cv2.imread(str(out_480)).shape[:2][::-1] != (w480, h480):\n",
        "            small = resize(img, (w480, h480))\n",
        "            cv2.imwrite(str(out_480), small, [cv2.IMWRITE_JPEG_QUALITY, 95])\n",
        "\n",
        "        # ------------- 1440-p -------------- #\n",
        "        out_1440 = DST_1440 / seq.name / img_path.name\n",
        "        if not out_1440.exists() or cv2.imread(str(out_1440)).shape[:2][::-1] != (w1440, h1440):\n",
        "            big = resize(img, (w1440, h1440))\n",
        "            cv2.imwrite(str(out_1440), big,  [cv2.IMWRITE_JPEG_QUALITY, 95])\n"
      ],
      "metadata": {
        "id": "zhrIAloFQw-A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from __future__ import annotations\n",
        "from pathlib import Path\n",
        "from typing import List, Tuple\n",
        "from PIL import Image\n",
        "from torch.utils.data import Dataset\n",
        "import torchvision.transforms as T\n",
        "import random\n",
        "\n",
        "SPLIT_RATIO = (0.70, 0.20, 0.10)\n",
        "_RANDOM_SEED = 42\n",
        "\n",
        "\n",
        "class TripletDataset(Dataset):\n",
        "    \"\"\"\n",
        "    Returns four tensors in [0,1], shape (C, H, W):\n",
        "        prev480, curr480, next480, hr1080\n",
        "    \"\"\"\n",
        "    def __init__(self,\n",
        "                 lr_root: str | Path,\n",
        "                 hr_root: str | Path,\n",
        "                 split: str = \"train\"):\n",
        "        assert split in {\"train\", \"val\", \"test\"}\n",
        "        lr_root, hr_root = Path(lr_root), Path(hr_root)\n",
        "        if not lr_root.exists() or not hr_root.exists():\n",
        "            raise FileNotFoundError(\"LR or HR root folder not found.\")\n",
        "\n",
        "        seq_names = sorted([d.name for d in lr_root.iterdir() if d.is_dir()])\n",
        "        random.Random(_RANDOM_SEED).shuffle(seq_names)\n",
        "\n",
        "        n = len(seq_names)\n",
        "        n_train = int(SPLIT_RATIO[0] * n)\n",
        "        n_val   = int(SPLIT_RATIO[1] * n)\n",
        "\n",
        "        if   split == \"train\": seq_names = seq_names[:n_train]\n",
        "        elif split == \"val\"  : seq_names = seq_names[n_train:n_train+n_val]\n",
        "        else                : seq_names = seq_names[n_train+n_val:]\n",
        "\n",
        "        self.samples: List[Tuple[Path, Path, Path, Path]] = []\n",
        "        self._to_tensor = T.ToTensor()\n",
        "\n",
        "        for seq in seq_names:\n",
        "            lr_frames = sorted((lr_root / seq).glob(\"*.jpg\"))\n",
        "            hr_frames = sorted((hr_root / seq).glob(\"*.jpg\"))\n",
        "            assert len(lr_frames) == len(hr_frames), f\"Mismatch in {seq}\"\n",
        "\n",
        "            for i in range(1, len(lr_frames) - 1):\n",
        "                self.samples.append(\n",
        "                    (lr_frames[i-1], lr_frames[i], lr_frames[i+1],\n",
        "                     hr_frames[i])\n",
        "                )\n",
        "\n",
        "    def __len__(self):  return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        p_prev, p_curr, p_next, p_hr = self.samples[idx]\n",
        "        return tuple(self._to_tensor(Image.open(p).convert(\"RGB\"))\n",
        "                     for p in (p_prev, p_curr, p_next, p_hr))\n"
      ],
      "metadata": {
        "id": "eFuO6zKVY3bi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3 Model Architecture & Definition\n",
        "\n",
        "This section translates our upscaling concept into **executable PyTorch code**, ready for\n",
        "quantization and hardware mapping.\n",
        "\n",
        "### 3.1 High-Level Snapshot\n",
        "* **Input block** → initial **3×3 conv** + ReLU to lift 3-channel RGB into the feature space.  \n",
        "* **Temporal core** → a stack of **Gated Convolutional Recurrent (GCR) layers** that carry hidden\n",
        "  state \\(h_{t-1} \\rightarrow h_{t}\\) and fuse motion cues frame-by-frame.  \n",
        "* **Upsampling head** → either  \n",
        "  1. **Pixel-shuffle** (sub-pixel) + **1×1 conv** to cut channels, or  \n",
        "  2. **Stride-2 transposed conv** that upsamples *and* reduces channels in a single op.  \n",
        "* **Skip connection** → adds the bilinearly upscaled seed to sharpen fine edges and stabilize\n",
        "  training.  \n",
        "\n",
        "### 3.2 Modules We Will Implement\n",
        "| Module | Purpose | Notes |\n",
        "|--------|---------|-------|\n",
        "| `ConvGRUCell2D` | Spatial GRU with 3×3 kernels | Hidden state kept in on-chip SRAM |\n",
        "| `CRNNBlock` | Stack of *N* GRU cells | Residual skip every 2 layers |\n",
        "| `UpsampleHead` | 2× upscale to 1440 p | Choice: pixel-shuffle **or** deconv |\n",
        "| `CRNNUpscaler` | Full end-to-end network |  ~0.9 M params @ 8-bit weights |\n",
        "\n",
        "### 3.3 Parameter & Hardware Budget\n",
        "* **Total MACs / 1080 p frame:** ≈ 2.1 G — fits a 256-MAC systolic array at 180 MHz, 30 fps.  \n",
        "* **SRAM footprint:**  \n",
        "  * Weights: **≈ 900 kB** (8-bit).  \n",
        "  * Hidden state: **≈ 256 kB** (64 × H/4 × W/4, 8-bit).  \n",
        "  * Line buffers: **≈ 1.6 MB** for 1-frame look-ahead (optional).  \n",
        "\n",
        "### 3.4 Config Knobs Exposed in Code\n",
        "* `N_GCR`: number of recurrent layers (depth vs. latency).  \n",
        "* `HIDDEN_C`: channel width of hidden state (quality vs. SRAM).  \n",
        "* `UPSAMPLE_MODE`: `\"pixelshuffle\"` or `\"deconv\"`.  \n",
        "* `QUANT_BITS`: {8, 6, 4} for exploration of power vs. PSNR trade-offs.\n",
        "\n",
        "---\n",
        "\n",
        "> **Next code cell:** implements the `ConvGRUCell2D` and builds the `CRNNUpscaler` class, followed by a\n",
        "> model summary (`torchinfo`) to verify tensor shapes and parameter counts.\n"
      ],
      "metadata": {
        "id": "wQiQdOMvZRyj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# ────────────────────────────────────────────────────────────────\n",
        "# 1. Conv-GRU cell (unchanged)\n",
        "# ────────────────────────────────────────────────────────────────\n",
        "class ConvGRUCell(nn.Module):\n",
        "    def __init__(self, in_c: int, hid_c: int, ks: int = 3):\n",
        "        super().__init__()\n",
        "        p = ks // 2\n",
        "        self.hid_c = hid_c\n",
        "        self.reset  = nn.Conv2d(in_c + hid_c, hid_c, ks, 1, p)\n",
        "        self.update = nn.Conv2d(in_c + hid_c, hid_c, ks, 1, p)\n",
        "        self.out    = nn.Conv2d(in_c + hid_c, hid_c, ks, 1, p)\n",
        "\n",
        "    def forward(self, x, h):\n",
        "        if h is None:\n",
        "            h = x.new_zeros(x.size(0), self.hid_c, x.size(2), x.size(3))\n",
        "        xc = torch.cat([x, h], 1)\n",
        "        r = torch.sigmoid(self.reset(xc))\n",
        "        z = torch.sigmoid(self.update(xc))\n",
        "        n = torch.tanh(self.out(torch.cat([x, r * h], 1)))\n",
        "        return (1 - z) * h + z * n\n",
        "\n",
        "\n",
        "# ────────────────────────────────────────────────────────────────\n",
        "# 2. Helper: a stack of Conv-GRU cells (no inter-layer state)\n",
        "# ────────────────────────────────────────────────────────────────\n",
        "class RecConvStack(nn.Module):\n",
        "    \"\"\"Sequential Conv-GRU layers; each layer gets a fresh hidden state.\"\"\"\n",
        "    def __init__(self, in_c: int, hid_list):\n",
        "        super().__init__()\n",
        "        cells, prev = [], in_c\n",
        "        for hid in hid_list:\n",
        "            cells.append(ConvGRUCell(prev, hid))\n",
        "            prev = hid\n",
        "        self.cells = nn.ModuleList(cells)\n",
        "\n",
        "    def forward(self, x):\n",
        "        for cell in self.cells:\n",
        "            x = cell(x, None)\n",
        "        return x\n",
        "\n",
        "\n",
        "# ────────────────────────────────────────────────────────────────\n",
        "# 3. Sub-pixel (pixel-shuffle) up-convolution\n",
        "# ────────────────────────────────────────────────────────────────\n",
        "class SubPixelBlock(nn.Sequential):\n",
        "    def __init__(self, in_c: int, out_c: int, scale: int):\n",
        "        super().__init__(\n",
        "            nn.Conv2d(in_c, out_c * scale * scale, 3, 1, 1),\n",
        "            nn.PixelShuffle(scale),\n",
        "            nn.ReLU(True)\n",
        "        )\n",
        "\n",
        "\n",
        "# ────────────────────────────────────────────────────────────────\n",
        "# 4. Triplet CRNN ×3 Upscaler\n",
        "# ────────────────────────────────────────────────────────────────\n",
        "class TripletCRNNx3(nn.Module):\n",
        "    \"\"\"\n",
        "    • Stem conv                        : 3  → 16  ch @ 480 p\n",
        "    • Two LR Conv-GRU layers           : 16 → 16  ch @ 480 p\n",
        "    • Sub-pixel upsample  (×3)         : 16 → 16  ch @ 1440 p\n",
        "    • Four HR Conv-GRU layers          : 16 → 32 → 16 → 8 → 3 ch\n",
        "    • Bilinear skip connection\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        # 0) stem\n",
        "        self.stem = nn.Sequential(\n",
        "            nn.Conv2d(3, 16, 3, 1, 1),  # (B,16,480,H)\n",
        "            nn.ReLU(True)\n",
        "        )\n",
        "\n",
        "        # 1) low-res CRNN trunk\n",
        "        self.lr_rec = RecConvStack(16, [16, 16])          # stays at 16 ch\n",
        "\n",
        "        # 2) up-convolution: 480 p → 1440 p, keep 16 ch\n",
        "        self.up = SubPixelBlock(16, 16, scale=3)\n",
        "\n",
        "        # 3) high-res CRNN trunk: 32 → 16 → 8 → 3\n",
        "        self.hr_rec = RecConvStack(16, [32, 16, 8, 3])\n",
        "\n",
        "    # ─── helper: run LR branch over (prev, curr, next) ──────────\n",
        "    def _run_lr_branch(self, frames):\n",
        "        h_list = [None, None]                              # for 2 LR layers\n",
        "        for f in frames:                                   # prev → curr → next\n",
        "            x = self.stem(f)\n",
        "            for i, cell in enumerate(self.lr_rec.cells):\n",
        "                h_list[i] = cell(x, h_list[i])\n",
        "                x = h_list[i]\n",
        "        return x                                           # (B,16,480,H)\n",
        "\n",
        "    # ─── forward ────────────────────────────────────────────────\n",
        "    def forward(self, prev, curr, nxt):\n",
        "        feat_lr = self._run_lr_branch([prev, curr, nxt])   # (B,16,480,H)\n",
        "        feat_hr = self.up(feat_lr)                         # (B,16,1440,3H)\n",
        "        sr_feat = self.hr_rec(feat_hr)                     # (B,3,1440,3H)\n",
        "\n",
        "        # bilinear residual\n",
        "        up_ref = F.interpolate(curr, scale_factor=3, mode=\"bilinear\",\n",
        "                               align_corners=False)\n",
        "        return (sr_feat + up_ref).clamp(0, 1)\n",
        "\n",
        "\n",
        "# # ────────────────── smoke test ──────────────────────────────────\n",
        "# if __name__ == \"__main__\":\n",
        "#     B, H = 1, 854\n",
        "#     lr = torch.randn(B, 3, 480, H)\n",
        "#     model = TripletCRNNx3()\n",
        "#     out = model(lr, lr, lr)\n",
        "#     print(\"Output shape:\", out.shape)      # → torch.Size([1, 3, 1440, 2562])\n"
      ],
      "metadata": {
        "id": "lEYmZCheZlOe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ╔════════════════════════════════════════════════════════════════════╗\n",
        "# ║  Train / test / demo harness – Jupyter edition (single GPU / CPU) ║\n",
        "# ╚════════════════════════════════════════════════════════════════════╝\n",
        "from types import SimpleNamespace\n",
        "from pathlib import Path\n",
        "from datetime import datetime\n",
        "import math, os, random, logging, sys\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "\n",
        "# ─── 1. Configuration block (edit in-notebook) ───────────────────────\n",
        "args = SimpleNamespace(\n",
        "    # I/O\n",
        "    data_root   = \"datasets\",\n",
        "    save_dir    = \"ckpt\",\n",
        "    weights     = None,          # path to .pt, or None\n",
        "    mode        = \"train\",       # \"train\" | \"test\" | \"demo\"\n",
        "    log_file    = \"training_psnr.log\",\n",
        "\n",
        "    # optimisation / schedule\n",
        "    epochs      = 2,\n",
        "    batch_size  = 4,\n",
        "    lr          = 2e-4,\n",
        "\n",
        "    # data-loader\n",
        "    num_workers = 2,\n",
        "\n",
        "    # demo\n",
        "    demo_samples = 3,\n",
        ")\n",
        "\n",
        "# ─── 2. Helpers unchanged from your script ───────────────────────────\n",
        "def add_timestamp(fname: str | Path) -> str:\n",
        "    ts = datetime.now().strftime(\"%d-%b_%H-%M\")\n",
        "    p  = Path(fname)\n",
        "    return str(p.with_name(f\"{p.stem}_{ts}{p.suffix or '.log'}\"))\n",
        "\n",
        "_YCOEF = torch.tensor([0.299, 0.587, 0.114]).view(1, 3, 1, 1)\n",
        "def rgb2y(t: torch.Tensor) -> torch.Tensor:\n",
        "    return (t * _YCOEF.to(t.device, t.dtype)).sum(1, keepdim=True)\n",
        "\n",
        "def psnr(pred: torch.Tensor, tgt: torch.Tensor, shave: int = 4) -> float:\n",
        "    pred, tgt = rgb2y(pred), rgb2y(tgt)\n",
        "    if shave:\n",
        "        pred = pred[..., shave:-shave, shave:-shave]\n",
        "        tgt  = tgt [..., shave:-shave, shave:-shave]\n",
        "    mse = F.mse_loss(pred, tgt, reduction=\"mean\")\n",
        "    return float(\"inf\") if mse == 0 else 10 * math.log10(1.0 / mse.item())\n",
        "\n",
        "@torch.no_grad()\n",
        "def validate(model, loader, device) -> float:\n",
        "    model.eval()\n",
        "    tot, n = 0.0, 0\n",
        "    for prev, cur, nxt, hr in loader:\n",
        "        prev, cur, nxt, hr = [t.to(device) for t in (prev, cur, nxt, hr)]\n",
        "        sr  = model(prev, cur, nxt).clamp(0, 1)\n",
        "        tot += psnr(sr, hr) * prev.size(0)\n",
        "        n   += prev.size(0)\n",
        "    return tot / n\n",
        "\n",
        "# ─── 3. Minimal logging for notebooks (prints + file) ────────────────\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO,\n",
        "    format=\"%(asctime)s [%(levelname)s] %(message)s\",\n",
        "    handlers=[\n",
        "        logging.FileHandler(add_timestamp(args.log_file), \"a\"),\n",
        "        logging.StreamHandler(sys.stdout)\n",
        "    ]\n",
        ")\n",
        "\n",
        "# ─── 4. Paths & dataset sanity check ─────────────────────────────────\n",
        "root   = Path(args.data_root)\n",
        "lr_dir = root / \"DAVIS_480\"\n",
        "hr_dir = root / \"DAVIS_1440\"\n",
        "assert lr_dir.exists() and hr_dir.exists(), \"→  Generate 480p/1440p first!\"\n",
        "\n",
        "train_set = TripletDataset(lr_dir, hr_dir, \"train\")\n",
        "val_set   = TripletDataset(lr_dir, hr_dir, \"val\")\n",
        "test_set  = TripletDataset(lr_dir, hr_dir, \"test\")\n",
        "\n",
        "train_ld = DataLoader(train_set, args.batch_size, shuffle=True,\n",
        "                      num_workers=args.num_workers, pin_memory=True)\n",
        "val_ld   = DataLoader(val_set, 1, shuffle=False,\n",
        "                      num_workers=args.num_workers, pin_memory=True)\n",
        "test_ld  = DataLoader(test_set, 1, shuffle=False,\n",
        "                      num_workers=args.num_workers, pin_memory=True)\n",
        "\n",
        "# ─── 5. Device & model ------------------------------------------------\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "torch.backends.cudnn.benchmark = True\n",
        "\n",
        "model = TripletCRNNx3().to(device)\n",
        "if args.weights:\n",
        "    model.load_state_dict(torch.load(args.weights, map_location=device))\n",
        "    logging.info(\"Loaded weights %s\", args.weights)\n",
        "\n",
        "opt = torch.optim.Adam(model.parameters(), lr=args.lr)\n",
        "Path(args.save_dir).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# ─── 6. Modes ---------------------------------------------------------\n",
        "if args.mode == \"test\":\n",
        "    print(f\"Test  PSNR: {validate(model, test_ld, device):.2f} dB\")\n",
        "elif args.mode == \"demo\":\n",
        "    from torchvision.utils import save_image\n",
        "    out_dir = Path(\"demo_out\"); out_dir.mkdir(exist_ok=True)\n",
        "    H, W, PATCH = 1440, 2562, 100\n",
        "    model.eval()\n",
        "    for idx in random.sample(range(len(test_set)), args.demo_samples):\n",
        "        prev, cur, nxt, hr = test_set[idx]\n",
        "        with torch.no_grad():\n",
        "            sr = model(prev.unsqueeze(0).to(device),\n",
        "                       cur .unsqueeze(0).to(device),\n",
        "                       nxt .unsqueeze(0).to(device))[0].cpu().clamp(0,1)\n",
        "        bilinear = F.interpolate(cur.unsqueeze(0), (H, W),\n",
        "                                 mode=\"bilinear\", align_corners=False)[0]\n",
        "        rows=[]\n",
        "        for _ in range(4):\n",
        "            y,x = random.randint(0,H-PATCH), random.randint(0,W-PATCH)\n",
        "            rows.append(torch.cat([bilinear[:,y:y+PATCH,x:x+PATCH],\n",
        "                                   sr      [:,y:y+PATCH,x:x+PATCH],\n",
        "                                   hr      [:,y:y+PATCH,x:x+PATCH]],2))\n",
        "        save_image(torch.cat(rows,1), out_dir/f\"demo_{idx:04d}.png\")\n",
        "    print(f\"Saved {args.demo_samples} demo grids to {out_dir.resolve()}\")\n",
        "else:  # ───────────── TRAIN ──────────────────────────────────────────\n",
        "    for epoch in range(1, args.epochs+1):\n",
        "        model.train(); epoch_loss=0\n",
        "        for prev,cur,nxt,hr in train_ld:\n",
        "            prev,cur,nxt,hr=[t.to(device) for t in (prev,cur,nxt,hr)]\n",
        "            opt.zero_grad(set_to_none=True)\n",
        "            sr   = model(prev,cur,nxt)\n",
        "            loss = F.l1_loss(sr,hr)\n",
        "            loss.backward(); opt.step()\n",
        "            epoch_loss += loss.item()*prev.size(0)\n",
        "        epoch_loss/=len(train_set)\n",
        "        psnr_val = validate(model,val_ld,device)\n",
        "        logging.info(\"Epoch %d  |  L1 %.4f  |  PSNR %.2f\",\n",
        "                     epoch, epoch_loss, psnr_val)\n",
        "        torch.save(model.state_dict(),\n",
        "                   Path(args.save_dir)/f\"epoch_{epoch:03d}.pt\")\n",
        "\n",
        "    logging.info(\"Finished training.\")\n"
      ],
      "metadata": {
        "id": "DtIlyHvh1f1g"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}