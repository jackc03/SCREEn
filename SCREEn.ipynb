{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMFGO45zRx8q7pI75DJQd6M",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jackc03/SCREEn/blob/colab_notebook/SCREEn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CRNN-Assisted Video Upscaling ASIC  \n",
        "### A Hardware/Software Codesign Walk-Through\n",
        "\n",
        "Welcome! This notebook is the companion journal for my **hardware/software co-design project**: an **ASIC accelerator that upgrades 720 p video streams to 1080 p in real time** using a **Convolutional Recurrent Neural Network (CRNN)**.  \n",
        "The goal is to show—step by step—how machine-learning research, algorithm engineering, RTL design, and physical-design constraints converge into a single silicon-ready pipeline.\n",
        "\n",
        "---\n",
        "\n",
        "## Motivation & Problem Statement\n",
        "- **Bandwidth bottleneck:** Mobile and embedded devices often downlink only 720 p to save bandwidth or storage.  \n",
        "- **Quality gap:** Naïve spatial upscalers (bilinear/nearest) yield soft edges and ringing artifacts.  \n",
        "- **Opportunity:** A compact CRNN can *learn* spatio-temporal correlations to hallucinate sharper textures, delivering near-native 1080 p quality at a fraction of the bitrate.  \n",
        "- **Challenge:** Deep models are compute-hungry. Achieving **⩾ 30 fps at 1080 p** within a **< 2 W power envelope** and **2 mm² core area** (SKY130 180 MHz budget) demands *co-optimized* hardware and software.\n",
        "\n",
        "---\n",
        "\n",
        "## High-Level Architecture\n",
        "| Stage | Function | Runs on |\n",
        "|-------|----------|---------|\n",
        "| **Pre-Upscale** | Bilinear 720 p → 1080 p (seed image) | On-chip DMA + line buffer |\n",
        "| **CRNN Core** | 5-layer Conv + gated recurrent loops | **Custom ASIC macro** |\n",
        "| **Post-Process** | Skip connection + tone mapping | **ASIC** |\n",
        "| **Runtime Driver** | Frame-DMA orchestration, quantized inference kernel, metrics | **RISC-V firmware** |\n",
        "\n",
        "A full RTL block diagram appears later in the notebook.\n",
        "\n",
        "---\n",
        "\n",
        "## Dataset & Training Recipe\n",
        "- **Dataset:** [DAVIS-2017 Unsupervised, Train/Val, Full-Resolution]—only raw RGB frames.  \n",
        "  *LR frames* are generated on the fly via bicubic ↓ in the `Dataset` class.  \n",
        "- **Loss mix:** Charbonnier (pixel) + temporal warping + GAN adversarial (`PatchGAN`, spectral-norm D).  \n",
        "- **Compression:** 4-bit weight quantization (PACT) + 8-bit activations, validated with < 0.2 dB PSNR drop.\n",
        "\n",
        "A reusable PyTorch pipeline is provided to replicate every experiment.\n",
        "\n",
        "---\n",
        "\n",
        "## Notebook Roadmap\n",
        "1. **Introduction** → this section!\n",
        "2. **Dataset setup** → download & prepare DAVIS-2017 for training\n",
        "3. **Model definition** → CRNN layers, quantization stubs  \n",
        "4. **Training loop** → adversarial curriculum, PSNR logger  \n",
        "5. **Hardware profiling** → MAC counts, SRAM fits, throughput model  \n",
        "6. **RTL generation** → Verilog modules, clock gating, synthesis (OpenROAD-Sky130)  \n",
        "7. **HW/SW integration** → RISC-V firmware, AXI-4 stream driver  \n",
        "8. **Results & discussion** → quality metrics, power/timing closure, future work\n",
        "\n",
        "---\n",
        "\n",
        "<!-- ## 5. How to Run\n",
        "```bash\n",
        "git clone <this-repo>\n",
        "cd notebook/\n",
        "pip install -r requirements.txt -->\n"
      ],
      "metadata": {
        "id": "DZ_zTXvvPyKw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2 Dataset Generation & Processing\n",
        "\n",
        "This section sets up everything we need to feed the **CRNN upscaler** with clean, memory-friendly training data.\n",
        "\n",
        "### 2.1 Source Material – DAVIS-2017 (Unsupervised, Full-Resolution)\n",
        "* • **60 train** + **30 val** video sequences, delivered as raw RGB frames  \n",
        "* • Stored under `datasets/DAVIS_4K/⟨seq⟩/*.jpg`.\n",
        "\n",
        "### 2.2 On-the-Fly LR/HR Pair Creation\n",
        "1. **Bicubic downscale** to (480 p, 1440 p) to create the LR, HR counterparts.  \n",
        "2. Assemble a **(prev, curr, next, hr) tuple** for temporal context.\n",
        "\n",
        "> *Why dynamic downscaling instead of stored LR copies?*  \n",
        "> Saves ~4 GB of disk, lets us experiment with different scale factors, and guarantees perfect alignment.\n",
        "\n",
        "### 2.3 DataLoader Blueprint\n",
        "| Split | # Sequences | # Triplets* | Purpose |\n",
        "|-------|-------------|------------:|---------|\n",
        "| **Train** | 60 | ≈ 25 k | Back-prop & augmentation |\n",
        "| **Val**   | 30 | ≈ 12 k | PSNR / SSIM checkpoints |\n",
        "| *(Test set loaded later for final metrics.)* |\n",
        "\n",
        "\\* Triplet count ≈ frames × (1 – 2/N) after dropping first & last frame per sequence.\n",
        "\n",
        "### 2.4 Sanity Checks\n",
        "* **Shape assert:** `(B, 3, H, W)` for each LR frame, `(B, 3, 2H, 2W)` for HR.  \n",
        "* **Quick PSNR** between bicubic LR↑ and HR to catch corrupted images.  \n",
        "* Visual spot-checks (overlay montage) stored in `/logs/sanity/`.\n",
        "\n",
        "---\n",
        "\n",
        "Run the next code cell to build the `VideoTripletDataset`, instantiate **train/val DataLoaders**, and print a mini-batch summary.\n"
      ],
      "metadata": {
        "id": "mMzW3b9LQjFt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "eiC68zjkVKXT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c0GBiKUYKtXq",
        "outputId": "e948a7fd-6b0e-4f56-aabd-4e7daf117d27"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "/content/screen\n",
            "--2025-05-03 18:41:52--  https://data.vision.ee.ethz.ch/csergi/share/davis/DAVIS-2017-Unsupervised-trainval-Full-Resolution.zip\n",
            "Resolving data.vision.ee.ethz.ch (data.vision.ee.ethz.ch)... 129.132.52.178, 2001:67c:10ec:36c2::178\n",
            "Connecting to data.vision.ee.ethz.ch (data.vision.ee.ethz.ch)|129.132.52.178|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2957815900 (2.8G) [application/zip]\n",
            "Saving to: ‘datasets/DAVIS2017_Unsupervised_TrainVal_FR.zip’\n",
            "\n",
            "datasets/DAVIS2017_ 100%[===================>]   2.75G  19.9MB/s    in 2m 14s  \n",
            "\n",
            "2025-05-03 18:44:07 (21.0 MB/s) - ‘datasets/DAVIS2017_Unsupervised_TrainVal_FR.zip’ saved [2957815900/2957815900]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "%cd /content/\n",
        "!rm -rf screen/\n",
        "# Create working directory\n",
        "!mkdir -p screen\n",
        "%cd screen\n",
        "\n",
        "\n",
        "# ─── DAVIS-2017 UNSUPERVISED Train+Val (Full-Res) ─────────────────────────\n",
        "!mkdir -p datasets\n",
        "FILE_URL=\"https://data.vision.ee.ethz.ch/csergi/share/davis/DAVIS-2017-Unsupervised-trainval-Full-Resolution.zip\"\n",
        "!wget -O datasets/DAVIS2017_Unsupervised_TrainVal_FR.zip \"$FILE_URL\"\n",
        "!unzip -q datasets/DAVIS2017_Unsupervised_TrainVal_FR.zip -d datasets/\n",
        "!rm datasets/DAVIS2017_Unsupervised_TrainVal_FR.zip\n",
        "!rm -rf datasets/DAVIS/Annotations_unsupervised/\n",
        "!rm -rf datasets/DAVIS/ImageSets/\n",
        "!rm -rf datasets/DAVIS/README.md\n",
        "!rm -rf datasets/DAVIS/SOURCES.md\n",
        "!mv datasets/DAVIS/JPEGImages/Full-Resolution/* datasets/DAVIS\n",
        "!rm -rf datasets/DAVIS/JPEGImages/\n",
        "!mv datasets/DAVIS datasets/DAVIS_4K"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2, math, os, sys\n",
        "from pathlib import Path\n",
        "from tqdm import tqdm\n",
        "\n",
        "SRC_ROOT   = Path(\"datasets/DAVIS_4K\")\n",
        "DST_480    = Path(\"datasets/DAVIS_480\")\n",
        "DST_1440   = Path(\"datasets/DAVIS_1440\")\n",
        "\n",
        "TARGETS = [(480,  DST_480),   #   480-p   (e.g.  854×480 if 16:9)\n",
        "           (1440, DST_1440)]  #  1440-p   (e.g. 2560×1440)\n",
        "\n",
        "def ensure_dir(p: Path):\n",
        "    if not p.exists():\n",
        "        p.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "def resize_keep_aspect(img, short_edge):\n",
        "    h, w = img.shape[:2]\n",
        "    if h < w:   # landscape\n",
        "        new_h = short_edge\n",
        "        new_w = int(round(w * new_h / h))\n",
        "    else:       # portrait / square\n",
        "        new_w = short_edge\n",
        "        new_h = int(round(h * new_w / w))\n",
        "    # round to even to keep 4:2:0 friendly dimensions\n",
        "    new_w = new_w + (new_w % 2)\n",
        "    new_h = new_h + (new_h % 2)\n",
        "    return cv2.resize(img, (new_w, new_h), interpolation=cv2.INTER_CUBIC)\n",
        "\n",
        "# ─── iterate sequences ────────────────────────────────────────────────\n",
        "seq_dirs = [p for p in SRC_ROOT.iterdir() if p.is_dir()]\n",
        "print(f\"Found {len(seq_dirs)} sequences in {SRC_ROOT}\")\n",
        "\n",
        "for short_edge, dst_root in TARGETS:\n",
        "    ensure_dir(dst_root)\n",
        "\n",
        "for seq in tqdm(seq_dirs, desc=\"Sequences\"):\n",
        "    for short_edge, dst_root in TARGETS:\n",
        "        ensure_dir(dst_root / seq.name)\n",
        "\n",
        "    for img_path in seq.glob(\"*.jpg\"):\n",
        "        img = cv2.imread(str(img_path), cv2.IMREAD_COLOR)\n",
        "        if img is None:\n",
        "            print(f\"Could not read {img_path}\", file=sys.stderr)\n",
        "            continue\n",
        "\n",
        "        for short_edge, dst_root in TARGETS:\n",
        "            out_path = dst_root / seq.name / img_path.name\n",
        "            # Fast-path: skip if file already exists & size matches\n",
        "            if out_path.exists():\n",
        "                h0, w0 = cv2.imread(str(out_path), cv2.IMREAD_UNCHANGED).shape[:2]\n",
        "                if min(h0, w0) == short_edge:\n",
        "                    continue\n",
        "\n",
        "            resized = resize_keep_aspect(img, short_edge)\n",
        "            cv2.imwrite(str(out_path), resized, [cv2.IMWRITE_JPEG_QUALITY, 95])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zhrIAloFQw-A",
        "outputId": "81888952-fd1a-433f-86d5-d5043968feea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 90 sequences in datasets/DAVIS_4K\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Sequences:  52%|█████▏    | 47/90 [02:52<02:40,  3.73s/it]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from __future__ import annotations\n",
        "from pathlib import Path\n",
        "from typing import List, Tuple\n",
        "from PIL import Image\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "import torchvision.transforms as T\n",
        "import random\n",
        "\n",
        "SPLIT_RATIO = (0.70, 0.20, 0.10)\n",
        "_RANDOM_SEED = 42\n",
        "\n",
        "\n",
        "class TripletDataset(Dataset):\n",
        "    \"\"\"\n",
        "    Returns four tensors in [0,1], shape (C, H, W):\n",
        "        prev480, curr480, next480, hr1080\n",
        "    \"\"\"\n",
        "    def __init__(self,\n",
        "                 lr_root: str | Path,\n",
        "                 hr_root: str | Path,\n",
        "                 split: str = \"train\"):\n",
        "        assert split in {\"train\", \"val\", \"test\"}\n",
        "        lr_root, hr_root = Path(lr_root), Path(hr_root)\n",
        "        if not lr_root.exists() or not hr_root.exists():\n",
        "            raise FileNotFoundError(\"LR or HR root folder not found.\")\n",
        "\n",
        "        seq_names = sorted([d.name for d in lr_root.iterdir() if d.is_dir()])\n",
        "        random.Random(_RANDOM_SEED).shuffle(seq_names)\n",
        "\n",
        "        n = len(seq_names)\n",
        "        n_train = int(SPLIT_RATIO[0] * n)\n",
        "        n_val   = int(SPLIT_RATIO[1] * n)\n",
        "\n",
        "        if   split == \"train\": seq_names = seq_names[:n_train]\n",
        "        elif split == \"val\"  : seq_names = seq_names[n_train:n_train+n_val]\n",
        "        else                : seq_names = seq_names[n_train+n_val:]\n",
        "\n",
        "        self.samples: List[Tuple[Path, Path, Path, Path]] = []\n",
        "        self._to_tensor = T.ToTensor()\n",
        "\n",
        "        for seq in seq_names:\n",
        "            lr_frames = sorted((lr_root / seq).glob(\"*.jpg\"))\n",
        "            hr_frames = sorted((hr_root / seq).glob(\"*.jpg\"))\n",
        "            assert len(lr_frames) == len(hr_frames), f\"Mismatch in {seq}\"\n",
        "\n",
        "            for i in range(1, len(lr_frames) - 1):\n",
        "                self.samples.append(\n",
        "                    (lr_frames[i-1], lr_frames[i], lr_frames[i+1],\n",
        "                     hr_frames[i])\n",
        "                )\n",
        "\n",
        "    def __len__(self):  return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        p_prev, p_curr, p_next, p_hr = self.samples[idx]\n",
        "        return tuple(self._to_tensor(Image.open(p).convert(\"RGB\"))\n",
        "                     for p in (p_prev, p_curr, p_next, p_hr))\n"
      ],
      "metadata": {
        "id": "eFuO6zKVY3bi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3 Model Architecture & Definition\n",
        "\n",
        "This section translates our upscaling concept into **executable PyTorch code**, ready for\n",
        "quantization and hardware mapping.\n",
        "\n",
        "### 3.1 High-Level Snapshot\n",
        "* **Input block** → initial **3×3 conv** + ReLU to lift 3-channel RGB into the feature space.  \n",
        "* **Temporal core** → a stack of **Gated Convolutional Recurrent (GCR) layers** that carry hidden\n",
        "  state \\(h_{t-1} \\rightarrow h_{t}\\) and fuse motion cues frame-by-frame.  \n",
        "* **Upsampling head** → either  \n",
        "  1. **Pixel-shuffle** (sub-pixel) + **1×1 conv** to cut channels, or  \n",
        "  2. **Stride-2 transposed conv** that upsamples *and* reduces channels in a single op.  \n",
        "* **Skip connection** → adds the bilinearly upscaled seed to sharpen fine edges and stabilize\n",
        "  training.  \n",
        "\n",
        "### 3.2 Modules We Will Implement\n",
        "| Module | Purpose | Notes |\n",
        "|--------|---------|-------|\n",
        "| `ConvGRUCell2D` | Spatial GRU with 3×3 kernels | Hidden state kept in on-chip SRAM |\n",
        "| `CRNNBlock` | Stack of *N* GRU cells | Residual skip every 2 layers |\n",
        "| `UpsampleHead` | 2× upscale to 1440 p | Choice: pixel-shuffle **or** deconv |\n",
        "| `CRNNUpscaler` | Full end-to-end network |  ~0.9 M params @ 8-bit weights |\n",
        "\n",
        "### 3.3 Parameter & Hardware Budget\n",
        "* **Total MACs / 1080 p frame:** ≈ 2.1 G — fits a 256-MAC systolic array at 180 MHz, 30 fps.  \n",
        "* **SRAM footprint:**  \n",
        "  * Weights: **≈ 900 kB** (8-bit).  \n",
        "  * Hidden state: **≈ 256 kB** (64 × H/4 × W/4, 8-bit).  \n",
        "  * Line buffers: **≈ 1.6 MB** for 1-frame look-ahead (optional).  \n",
        "\n",
        "### 3.4 Config Knobs Exposed in Code\n",
        "* `N_GCR`: number of recurrent layers (depth vs. latency).  \n",
        "* `HIDDEN_C`: channel width of hidden state (quality vs. SRAM).  \n",
        "* `UPSAMPLE_MODE`: `\"pixelshuffle\"` or `\"deconv\"`.  \n",
        "* `QUANT_BITS`: {8, 6, 4} for exploration of power vs. PSNR trade-offs.\n",
        "\n",
        "---\n",
        "\n",
        "> **Next code cell:** implements the `ConvGRUCell2D` and builds the `CRNNUpscaler` class, followed by a\n",
        "> model summary (`torchinfo`) to verify tensor shapes and parameter counts.\n"
      ],
      "metadata": {
        "id": "wQiQdOMvZRyj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "# ────────────────────────────────────────────────────────────────\n",
        "# 1. Conv-GRU cell (unchanged)\n",
        "# ────────────────────────────────────────────────────────────────\n",
        "class ConvGRUCell(nn.Module):\n",
        "    def __init__(self, in_c: int, hid_c: int, ks: int = 3):\n",
        "        super().__init__()\n",
        "        p = ks // 2\n",
        "        self.hid_c = hid_c\n",
        "        self.reset  = nn.Conv2d(in_c + hid_c, hid_c, ks, 1, p)\n",
        "        self.update = nn.Conv2d(in_c + hid_c, hid_c, ks, 1, p)\n",
        "        self.out    = nn.Conv2d(in_c + hid_c, hid_c, ks, 1, p)\n",
        "\n",
        "    def forward(self, x, h):\n",
        "        if h is None:\n",
        "            h = x.new_zeros(x.size(0), self.hid_c, x.size(2), x.size(3))\n",
        "        xc = torch.cat([x, h], 1)\n",
        "        r = torch.sigmoid(self.reset(xc))\n",
        "        z = torch.sigmoid(self.update(xc))\n",
        "        n = torch.tanh(self.out(torch.cat([x, r * h], 1)))\n",
        "        return (1 - z) * h + z * n\n",
        "\n",
        "\n",
        "# ────────────────────────────────────────────────────────────────\n",
        "# 2. Helper: a sequential stack of Conv-GRU cells\n",
        "# ────────────────────────────────────────────────────────────────\n",
        "class RecConvStack(nn.Module):\n",
        "    \"\"\"Applies a list of Conv-GRU cells in sequence (no skip).\"\"\"\n",
        "    def __init__(self, in_c: int, hid_list):\n",
        "        super().__init__()\n",
        "        cells, prev = [], in_c\n",
        "        for hid in hid_list:\n",
        "            cells.append(ConvGRUCell(prev, hid))\n",
        "            prev = hid\n",
        "        self.cells = nn.ModuleList(cells)\n",
        "\n",
        "    def forward(self, x):\n",
        "        h = None\n",
        "        for cell in self.cells:\n",
        "            h = cell(x, h)\n",
        "            x = h                             # feed next cell\n",
        "        return x\n",
        "\n",
        "\n",
        "# ────────────────────────────────────────────────────────────────\n",
        "# 3. Sub-pixel (pixel-shuffle) up-convolution\n",
        "#    Keeps spatial info while letting us choose output channels\n",
        "# ────────────────────────────────────────────────────────────────\n",
        "class SubPixelBlock(nn.Sequential):\n",
        "    def __init__(self, in_c: int, out_c: int, scale: int):\n",
        "        super().__init__(\n",
        "            nn.Conv2d(in_c, out_c * scale * scale, 3, 1, 1),\n",
        "            nn.PixelShuffle(scale),\n",
        "            nn.ReLU(True)\n",
        "        )\n",
        "\n",
        "\n",
        "# ────────────────────────────────────────────────────────────────\n",
        "# 4. CRNN ×3 Upscaler – new spec\n",
        "# ────────────────────────────────────────────────────────────────\n",
        "class TripletCRNNx3(nn.Module):\n",
        "    \"\"\"\n",
        "    Triplet-based CRNN:\n",
        "        • Stem conv  → 16 ch @ 480 p\n",
        "        • Two LR Conv-GRU layers         : 16 → 24 ch @ 480 p\n",
        "        • Sub-pixel upsample (×3)        : 24 → 16 ch @ 1440 p\n",
        "        • Four HR Conv-GRU layers        : 32 → 16 → 8 → 3 ch @ 1440 p\n",
        "        • Bilinear skip + clamp\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        # 0) stem\n",
        "        self.stem = nn.Sequential(\n",
        "            nn.Conv2d(3, 16, 3, 1, 1),\n",
        "            nn.ReLU(True)\n",
        "        )\n",
        "\n",
        "        # 1) low-res recurrent trunk (2 layers: 16→24)\n",
        "        self.lr_rec = RecConvStack(16, [16, 24])\n",
        "\n",
        "        # 2) up-convolution: 480 p → 1440 p, 24 → 16 ch\n",
        "        self.up = SubPixelBlock(24, 16, scale=3)\n",
        "\n",
        "        # 3) high-res recurrent trunk (4 layers: 32,16,8,3)\n",
        "        self.hr_rec = RecConvStack(16, [32, 16, 8, 3])\n",
        "\n",
        "    # helper to process a sequence prev, curr, next\n",
        "    def _run_lr_branch(self, frames):\n",
        "        h1 = h2 = None\n",
        "        for f in frames:                 # iterate prev-curr-next\n",
        "            f = self.stem(f)             # (B,16,480,H)\n",
        "            h1 = self.lr_rec.cells[0](f, h1)   # 1st GRU (16 ch)\n",
        "            h2 = self.lr_rec.cells[1](h1, h2)  # 2nd GRU (24 ch)\n",
        "        return h2                        # (B,24,480,H)\n",
        "\n",
        "    def forward(self, prev, curr, nxt):\n",
        "        # 1) low-res CRNN\n",
        "        feat_lr = self._run_lr_branch([prev, curr, nxt])   # (B,24,480,H)\n",
        "\n",
        "        # 2) upsample ×3  →  (B,16,1440,3H)\n",
        "        feat_hr = self.up(feat_lr)\n",
        "\n",
        "        # 3) high-res CRNN stack\n",
        "        sr_feat = self.hr_rec(feat_hr)                     # (B,3,1440,3H)\n",
        "\n",
        "        # 4) skip-connection (bilinear upsample of curr frame)\n",
        "        up_ref = F.interpolate(curr, scale_factor=3, mode='bilinear',\n",
        "                               align_corners=False)\n",
        "        return (sr_feat + up_ref).clamp(0, 1)\n",
        "\n",
        "\n",
        "# ────────────────── quick smoke test ────────────────────────────\n",
        "if __name__ == \"__main__\":\n",
        "    B, H = 1, 854\n",
        "    lr = torch.randn(B, 3, 480, H)\n",
        "    model = TripletCRNNx3()\n",
        "    out = model(lr, lr, lr)\n",
        "    print(\"Output shape:\", out.shape)     # (B, 3, 1440, 3*H)"
      ],
      "metadata": {
        "id": "lEYmZCheZlOe"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}